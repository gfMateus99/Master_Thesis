{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Pre-processing Text.ipynb","provenance":[{"file_id":"https://github.com/stanfordnlp/stanza/blob/master/demo/Stanza_Beginners_Guide.ipynb","timestamp":1654298550631}],"collapsed_sections":["EWZRmS09uPwK","yQff4Di5Nnq0","zOipgN1kocWF","iZDaXjjimDLp","I-nvmiYGoywp","YJ2hi5nwpeZL","cXubyHJ2qCeZ","juIAPHrXqiS7","Co-A8Nwatvza","bZZHizUIt-9X","0OSDunU7wZCh"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"56LiYCkPM7V_"},"source":["## **Pre-Processing for text analysis** \n","\n","**@author:** Gonçalo Mateus\n","\n","**Note:** In case of using this script, you will have to import the initial file in \"1. Importing Data\" and be aware that your file need to have the camp with sentences to be analyzed with name \"text\". Moreover you need to import the stopwords file, and the sentilex-pt-.flex file.\n","\n","\n"]},{"cell_type":"markdown","source":["## **0. Mount Drive access**"],"metadata":{"id":"EWZRmS09uPwK"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"JHpsegYVf9HC","executionInfo":{"status":"ok","timestamp":1654677248584,"user_tz":-60,"elapsed":41077,"user":{"displayName":"Gonçalo Mateus","userId":"12277450547086750918"}},"outputId":"5f215e6a-25d6-452f-db5d-df5aeb907995","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"yQff4Di5Nnq0"},"source":["## **1. Importing Data**\n"]},{"cell_type":"code","source":["\n","import pandas as pd\n","import seaborn as sns\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from datetime import datetime\n","import numpy as np\n","\n","import re\n","\n","#In case of run this this path should be redefined for the same directory of your file\n","df = pd.read_csv(r'/content/drive/MyDrive/AllMediaTweets_DF(Filter by keywords) - Sentiment_ALL.csv', \n","                  low_memory=False)      \n","\n","#df.drop(\"Unnamed: 0\", inplace=True, axis=1)\n","#df.drop(\"Unnamed: 0.1\", inplace=True, axis=1)\n","#df.drop(\"Unnamed: 0.1.1\", inplace=True, axis=1)\n","\n","df    \n"],"metadata":{"id":"G1Ubfv2egKcE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **2. Pre-Processing Text Steps**\n","\n"],"metadata":{"id":"zOipgN1kocWF"}},{"cell_type":"markdown","source":["### 2.1 Cleaning Text\n","\n"],"metadata":{"id":"iZDaXjjimDLp"}},{"cell_type":"code","source":["remove_rt = lambda x: re.sub('RT @\\w+: ',\" \", x)\n","rt = lambda x: re.sub(\"(@[A-Za-z0-9]+)|([^-,%0-9A-ZÁÀÂÃÉÈÊÍÌÓÒÔÕÚÇa-záàâãéèêíìóòôõúç \\t])|(\\w+:\\/\\/\\S+)\",\" \",x)\n","remove_double_space = lambda x: re.sub(\"  \",\" \", x)\n","rt2 = lambda x: re.sub(\"([,0-9A \\t])\",\" \",x)\n","\n","removed = df.text.map(remove_rt).map(rt).map(remove_double_space).map(rt2)\n","text_cleaned = removed.str.lower()\n","\n","text_cleaned_df = pd.DataFrame(text_cleaned)    \n","\n","df[\"text_cleaned\"] = text_cleaned"],"metadata":{"id":"36RBCg4kmG7x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2.2 Tokenization"],"metadata":{"id":"I-nvmiYGoywp"}},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import word_tokenize\n","from tqdm import tqdm\n","\n","text = list(map(lambda x: re.sub(r'\\b\\w{1,3}\\b', '', x), df[\"text_cleaned\"]))  \n","\n","rt = lambda x: re.sub(\"(@[A-Za-z0-9]+)|([^,%0-9A-ZÁÀÂÃÉÈÊÍÌÓÒÔÕÚÇa-záàâãéèêíìóòôõúç \\t])|(\\w+:\\/\\/\\S+)\",\" \",x)\n","remove_double_space = lambda x: re.sub(\"  \",\" \", x)\n","removed = df.text_cleaned.map(rt).map(remove_double_space)\n","\n","df[\"text_cleaned\"] = removed\n","\n","tokenization = list(map(lambda x: word_tokenize(x), df[\"text_cleaned\"]))\n","\n","df[\"Tokenization\"] = tokenization\n","\n","\n","df.head()    \n"],"metadata":{"id":"KnkoKf84gNJO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2.3 Stopwords removal and Remove small words(>2 characters)\n","\n","To remove Stopwords we useLX-Stopwords, a Portuguese dictionary composed of 2631 words created by the Natural Language and Speech Group of the University of Lisbon (The NLX-Group). \n","\n","URL: https://portulanclarin.net/repository/browse/lx-stopwords/29892e16a35a11e1a404080027e73ea22e53349e39f348a7944b0b5bef6e9c41/\n","\n"],"metadata":{"id":"YJ2hi5nwpeZL"}},{"cell_type":"code","source":["stopwords = pd.read_csv(r'/content/drive/MyDrive/LX-Stopwords/Stopwords.csv', low_memory=False)\n","stopwords.drop(\"class\", inplace=True, axis=1)\n","stopwords.drop(\"entries/0/sub-class\", inplace=True, axis=1)\n","\n","\n","stopwords_list = np.array(np.concatenate((stopwords.iloc[0].values, stopwords.iloc[1].values)))\n","stopwords_list = pd.DataFrame(stopwords_list)\n","stopwords_list = stopwords_list[stopwords_list[0].notna()]\n","stopwords_list = stopwords_list[~stopwords_list[0].str.contains(\"_\") == True]\n","\n","#replace the a a by à and a as by às\n","stopwords_list = list(map(lambda x: re.sub('^a a ', 'à ', x), stopwords_list[0]))\n","stopwords_list = list(map(lambda x: re.sub('^a as ', 'à ', x), stopwords_list))  \n","stopwords_list.append(\"que\")\n","stopwords_list.append(\"está\")\n","stopwords_list.append(\"'\")\n","stopwords_list.append(\"para\")\n","stopwords_list.append(\",\")\n","stopwords_list.append(\"para novo\")\n","stopwords_list.append(\"este\")\n","stopwords_list.append(\"como\")\n","stopwords_list.append(\"diz\")\n","stopwords_list.append(\"dos\")\n","\n","stopwords_removal = []       \n","\n","for value in df['Tokenization']:\n","    toaddarray = []\n","\n","    for x in value:\n","      if(x in stopwords_list):\n","        if(x == \"obrigado\"):\n","          toaddarray.append(x)\n","      else:\n","        if(len(x) > 2):\n","          toaddarray.append(x)\n","    \n","    stopwords_removal.append(toaddarray)\n","        \n","        \n","df[\"Remove stopwords\"] = stopwords_removal\n","df.head()   \n","        \n","        \n","        \n","        \n","        \n","        \n","        "],"metadata":{"id":"AI-gc2d6iHbu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 2.4 Obtaining the stem words and pos tagging\n","\n","These steps consist of studying the words more grammatically, and were done using SentiLex-PT. In lack of words cases in this dictionary, we used an NLP package made for different languages named Stanza.\n","\n","Part-of-speech (POS) tagging will associate a label indicating the grammatical category it belongs to, and the last step will convert each word to its stem form.\n","\n","To obtain the stem words there are two popular techniques: stemming and lemmatization. In this work, we used lemmatization. Lemmatization uses vocabulary and morphological analysis to identify the inflected forms of words, and studies show that it can provide better results when compared to stemming. "],"metadata":{"id":"152Y2shnnSIB"}},{"cell_type":"markdown","source":["#### 2.4.1 Installing Stanza\n","\n","Stanza is a Python NLP toolkit that supports 60+ human languages. It is built with highly accurate neural network components that enable efficient training and evaluation with your own annotated data, and offers pretrained models on 100 treebanks. Additionally, Stanza provides a stable, officially maintained Python interface to Java Stanford CoreNLP Toolkit.\n","\n"],"metadata":{"id":"cXubyHJ2qCeZ"}},{"cell_type":"code","source":["# Install; note that the prefix \"!\" is not needed if you are running in a terminal\n","!pip install stanza\n","\n","# Import the package\n","import stanza\n","\n","# Download the Portuguese model into the default directory\n","print(\"Downloading Portuguese model...\")\n","stanza.download('pt')\n","\n","# Build an Portuguese pipeline, with all processors by default\n","print(\"Building an Portuguese pipeline...\")\n","pt_nlp = stanza.Pipeline('pt')\n","\n"],"metadata":{"id":"bvh-AaFOmZ_q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2.4.2 Test Stanza"],"metadata":{"id":"juIAPHrXqiS7"}},{"cell_type":"code","source":["en_doc = pt_nlp(\"Banco vai aumentar dívida\")\n","word = en_doc.sentences[0].words[0]\n","print(word.lemma)\n","print(word.upos)\n","print(word)"],"metadata":{"id":"lFYaKXgNHi7r"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2.4.3 Importing SentiLex"],"metadata":{"id":"Co-A8Nwatvza"}},{"cell_type":"code","source":["sentilex_flex = pd.read_csv(r'/content/drive/MyDrive/SentiLex-PT02/SentiLex-flex-PT02.txt', delimiter=\".\",\n","                   header=None)\n","\n","split_words = sentilex_flex[0].str.split(',', expand=True)\n","classify = sentilex_flex[1].str.split(';', expand=True)\n","sentilex_dataframe = pd.DataFrame(split_words)      \n","sentilex_dataframe[\"polNo\"] = classify[3].str.split('=', expand=True)[1] #polarity\n","sentilex_dataframe[\"pOs\"] = classify[0].str.split('=', expand=True)[1] #idiomatic\n","\n","sentilex_dataframe"],"metadata":{"id":"IKHe3157tu3m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 2.4.4 Find stopwords and pos tagging\n","\n","This next step can take a while"],"metadata":{"id":"bZZHizUIt-9X"}},{"cell_type":"code","source":["lemmaarray = []\n","pos_tagging = []\n","count = 0\n","for value in df['Remove stopwords']:\n","    print(str(count)+\"/\"+str(len(df['Remove stopwords'])))\n","    count = count + 1;\n","\n","    toaddlemma = []\n","    toaddpos_tags = []\n","\n","    for x in value:\n","      pt_doc = pt_nlp(x)\n","\n","      word = pt_doc.sentences[0].words[0]\n","      #print(word)\n","\n","      isInSentilex = sentilex_dataframe[sentilex_dataframe[0] == word]\n","\n","      #Search first in Sentilex for the tag\n","      tag = \"\"\n","      if(len(isInSentilex) != 0):\n","        if(len(isInSentilex) != 0):\n","          if(len(isInSentilex) > 1):\n","            tag = isInSentilex[:1][1]\n","          else:\n","            tag = isInSentilex[\"pOs\"]             \n","\n","      #if not found tag in sentilex, put tag of stanza\n","      if(tag != \"\"):\n","        toaddpos_tags.append([word.text, tag])\n","        toaddlemma.append(isInSentilex[:1][\"pOs\"])\n","      else:\n","        toaddpos_tags.append([word.text, word.upos])\n","        toaddlemma.append(word.lemma)\n","      \n","           \n","    lemmaarray.append(toaddlemma)\n","    pos_tagging.append(toaddpos_tags) \n","        \n","df[\"Lemmatization\"] = lemmaarray\n","df[\"POS tagging\"] = pos_tagging\n","\n","df.head()   \n","\n","df.to_csv('/content/drive/MyDrive/Colab Notebooks/AllMediaTweets_DF(Filter by keywords) - Sentiment_ALL(with preprocess).csv')\n","\n","        "],"metadata":{"id":"uh7K9RiFsC0U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **3 Save new file**"],"metadata":{"id":"0OSDunU7wZCh"}},{"cell_type":"code","source":["df.to_csv('/content/drive/MyDrive/Colab Notebooks/AllMediaTweets_DF(Filter by keywords) - Sentiment_ALL(with preprocess).csv')\n"],"metadata":{"cellView":"code","id":"kEJ1xyA8sV-f"},"execution_count":null,"outputs":[]}]}