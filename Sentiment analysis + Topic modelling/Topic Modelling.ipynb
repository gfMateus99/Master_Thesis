{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["MZzUJKcgyhuH","p4DL1MpkyjSI","sbJEzwnVyjng","LlYSiMaEzB5a","GafYTKUezISR"],"authorship_tag":"ABX9TyP/rlHvqeKpjEzKITb7jZDG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## **Topic Modeling** \n","\n","**@author:** Gonçalo Mateus"],"metadata":{"id":"Lj3M3n7tyXSA"}},{"cell_type":"markdown","source":["## **0. Mount Drive access**"],"metadata":{"id":"MZzUJKcgyhuH"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"DBplkWc0yWtx"},"outputs":[],"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"]},{"cell_type":"markdown","source":["## **1. Importing Data**\n"],"metadata":{"id":"p4DL1MpkyjSI"}},{"cell_type":"code","source":["#--------------------------------------------#\n","#                   Imports                  #\n","#--------------------------------------------#\n","\n","import pandas as pd\n","import seaborn as sns\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from datetime import datetime\n","import numpy as np\n","import re\n","\n","\n","#--------------------------------------------#\n","#                 Load Data                  #\n","#--------------------------------------------#\n","\n","df = pd.read_csv(r'/content/drive/MyDrive/Scripts/Sentiment Analysis and Topic Modelling Study/Topic Modeling/AllMediaTweets_DF(Filter by keywords) - Sentiment_ALL(with_preprocess).csv', \n","#                  parse_dates=['created_at'],\n","#                  date_parser=custom_date_parser,\n","                  low_memory=False)\n","\n"],"metadata":{"id":"2ORoAdngyjZx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **2. Initial Process**\n"],"metadata":{"id":"sbJEzwnVyjng"}},{"cell_type":"code","source":["from ast import literal_eval\n","df['Lemmatization'] = df['Lemmatization'].apply(literal_eval)\n","df[\"final_process\"] = list(map(lambda x: ' '.join([str(elem) for elem in x]) , df['Lemmatization']))\n","\n","df[\"final_process_novobanco_out\"] = list(map(lambda x: x.replace('novo banco', ''), df['final_process']))\n","df[\"final_process_novobanco_out\"] = list(map(lambda x: x.replace('novobanco', ''), df['final_process_novobanco_out']))\n","df[\"final_process_novobanco_out\"] = list(map(lambda x: x.replace('dos', ''), df['final_process_novobanco_out']))\n","\n","remove_rt = lambda x: re.sub('RT @\\w+: ',\" \", x)\n","rt = lambda x: re.sub(\"([,0-9A \\t])\",\" \",x)\n","\n","df[\"new_text\"] = df.final_process_novobanco_out.map(remove_rt).map(rt)\n","\n","def remove_accents(raw_text):\n","    \"\"\"Removes common accent characters.\n","    \"\"\"\n","\n","    raw_text = re.sub(u\"[àáâãäå]\", 'a', raw_text)\n","    raw_text = re.sub(u\"[èéêë]\", 'e', raw_text)\n","    raw_text = re.sub(u\"[ìíîï]\", 'i', raw_text)\n","    raw_text = re.sub(u\"[òóôõö]\", 'o', raw_text)\n","    raw_text = re.sub(u\"[ùúûü]\", 'u', raw_text)\n","    raw_text = re.sub(u\"[ýÿ]\", 'y', raw_text)\n","    raw_text = re.sub(u\"[ß]\", 'ss', raw_text)\n","    raw_text = re.sub(u\"[ñ]\", 'n', raw_text)\n","    return raw_text \n","\n","alltext = df[\"new_text\"] \n","newtext = []\n","\n","for x in alltext:\n","  newstring = remove_accents(x)\n","  newtext.append(newstring)\n","\n","df[\"new_text\"]  = newtext\n","df[\"new_text\"] = list(map(lambda x: x.replace('  ', ' '), df['new_text']))\n","df[\"new_text\"] = list(map(lambda x: x.replace('  ', ' '), df['new_text']))\n","\n","numberWords = []\n","for x in df[\"new_text\"]:\n","    numberWords.append(len(x.split()))\n","\n","df[\"numberOfWords\"] = numberWords\n","df = df[df[\"numberOfWords\"] > 0]\n","\n","df[\"new_text\"].to_csv(r'/content/drive/MyDrive/Scripts/Sentiment Analysis and Topic Modelling Study/Topic Modeling/Data_to_topic1.txt', header=None, index=None,)"],"metadata":{"id":"PCLb0yUUyjtY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **3. Statistics**\n"],"metadata":{"id":"LlYSiMaEzB5a"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","countVectorizer = CountVectorizer() \n","countVector = countVectorizer.fit_transform(df[\"new_text\"])\n","print('{} Number of reviews has {} words'.format(countVector.shape[0], countVector.shape[1]))\n","\n","#print(countVectorizer.get_feature_names())\n","\n","count_vect_df = pd.DataFrame(countVector.toarray(), columns=countVectorizer.get_feature_names())\n","print(count_vect_df.head())\n","\n","count = pd.DataFrame(count_vect_df.sum())\n","countdf = count.sort_values(0,ascending=False).head(30)\n","print(countdf[0:21])\n","countdf = countdf.reset_index()\n","\n","#%% Top 20 words occurrences in tweets related to novobanco\n","plt.figure( figsize = ( 14, 7))\n","\n","ax = sns.barplot(x=\"index\", y=0, data=countdf[0:20],\n","                 palette=\"Blues_r\")\n","\n","plt.xticks(rotation=60)\n","plt.title(\"Top 20 words occurrences in tweets related to novobanco\", size=18)\n","plt.xlabel('Words', size=14)\n","plt.ylabel('Number of occurrences', size=14)\n","plt.xticks(size=13)\n","plt.yticks(size=13)"],"metadata":{"id":"H4guH0fqy-ph"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **4. Topic modeling using DMM with Gibbs Sampling**\n","\n","**Note:** For this part we used an implementation present in https://github.com/CAIR-ZA/GPyM_TM"],"metadata":{"id":"GafYTKUezISR"}},{"cell_type":"code","source":["!pip install GPyM-TM==3.0.0\n"],"metadata":{"id":"uAALVkfnzLUB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from GPyM_TM import GSDMM\n","import pandas as pd\n","\n","\n","def runTopic(nTopics, alpha, beta, nTopWords, iters, resultsy, topicCoehrence):\n","  corpus = GSDMM.load_file(\"/content/drive/MyDrive/Scripts/Sentiment Analysis and Topic Modelling Study/Topic Modeling/Data_to_topic.txt\")\n","\n","  #data_dmm = GSDMM.DMM(corpus, nTopics) # Initialize the object, with default parameters.\n","  #data_dmm = GSDMM.DMM(corpus, nTopics, alpha, beta, nTopWords, iters) # Initialize the object.\n","\n","  data_dmm = GSDMM.DMM(corpus, nTopics = nTopics, alpha = alpha, beta = beta, nTopWords = nTopWords, iters=iters)\n","\n","\n","\n","  data_dmm.topicAssigmentInitialise() # Performs the inital document assignments and counts\n","  data_dmm.inference()\n","\n","  psi, theta, selected_psi, selected_theta = data_dmm.worddist() # Determines and stores the psi, theta and selected_psi and selected_theta values\n","    \n","  finalAssignments = data_dmm.writeTopicAssignments() # Records the final topic assignments for the documents\n","\n","  coherence_topwords = data_dmm.writeTopTopicalWords(finalAssignments) # Record the top words for each document\n","\n","  score = data_dmm.coherence(coherence_topwords, len(finalAssignments)) #Calculates and stores the coherence\n","\n","  print(\"Final number of topics found: \" + str(len(finalAssignments)))\n","  \n","  resultsy.append(len(finalAssignments))\n","  topicCoehrence.append(score)\n","\n","  # We can then have to variables in which the selected theta's and psi are saved\n","  selected_psi\n","  selected_theta\n","\n","\n","#starting k\n","#paramsToChoose = [1, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 150, 200, 250, 300, 400, 500, 600, 700]\n","\n","paramsToChoose = [0.7]\n","\n","resultsx = []\n","resultsy = []\n","topicCoehrence = []\n","\n","for x in paramsToChoose:\n","  resultsx.append(x)\n","  nTopics = 400\n","\n","  alpha = x\n","  beta = 0.3\n","  nTopWords = 10\n","  iters = 100\n","  runTopic(nTopics, alpha, beta, nTopWords, iters, resultsy, topicCoehrence);\n","\n","#Save\n","title = \"Influence of iteractions\"\n","data = {'startingK':  resultsx,\n","        'endingK': resultsy,\n","        'coeherence': topicCoehrence,\n","        }\n","\n","data = pd.DataFrame(data)\n","\n","data.to_csv(r'/content/drive/MyDrive/Scripts/Sentiment Analysis and Topic Modelling Study/Topic Modeling/'+ title + '.csv')"],"metadata":{"id":"jCMMiy2JzNjC"},"execution_count":null,"outputs":[]}]}